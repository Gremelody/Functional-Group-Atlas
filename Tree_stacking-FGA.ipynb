{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbab798e-985c-4144-9b26-aba9fec746b7",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf54188-22e6-4b7a-bb52-1988283d4f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- GLOBAL CONFIGURATION & PARAMETERS ---\n",
    "# ==============================================================================\n",
    "# System-wide configuration and adjustable parameters.\n",
    "\n",
    "# --- 1. Data Source and Feature Engineering ---\n",
    "# File path for the input dataset (Excel format).\n",
    "EXCEL_FILE_PATH = 'Final_engineered_dataset-FGA-Eb.xlsx'\n",
    "# EXCEL_FILE_PATH = 'Final_engineered_dataset-FGA-NVOA.xlsx'\n",
    "\n",
    "# Feature (X) and Target (Y) column slicing definitions.\n",
    "# slice(start, stop) -> 'start' is inclusive, 'stop' is exclusive.\n",
    "X_COLS_SLICE = slice(1, -1)  # Features: From index 1 to the second-to-last column.\n",
    "Y_COLS_SLICE = -1            # Target: The last column.\n",
    "\n",
    "# --- 2. Cross-Validation Strategy ---\n",
    "CV_N_SPLITS = 10         # Number of folds for K-Fold cross-validation.\n",
    "CV_SHUFFLE = True        # Enable data shuffling prior to splitting.\n",
    "CV_RANDOM_STATE = 100    # Random seed for reproducibility of splits.\n",
    "\n",
    "# --- 3. Bayesian Optimization Configuration ---\n",
    "N_ITER_BAYESIAN = 30     # Number of optimization iterations (sampling steps).\n",
    "\n",
    "# --- 4. Global Model Initialization ---\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0  # Global random state for estimator initialization.\n",
    "\n",
    "# --- 5. Active Model Selection ---\n",
    "# Define the ensemble of models to be executed in the pipeline.\n",
    "# Uncomment a model ID to enable it; comment out to disable.\n",
    "ENABLED_MODELS = [\n",
    "    'XGBR',   # XGBoost Regressor\n",
    "    'RF',     # Random Forest Regressor\n",
    "    'GBRT',   # Gradient Boosting Regressor\n",
    "    'HGBR',   # Histogram-based Gradient Boosting Regressor\n",
    "    'ETR',    # Extra Trees Regressor\n",
    "    'CBR',    # CatBoost Regressor\n",
    "    'LGBM',   # LightGBM Regressor\n",
    "]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- HYPERPARAMETER SEARCH SPACES ---\n",
    "# ==============================================================================\n",
    "# Definition of the search space for Bayesian optimization for each estimator.\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "parameter_XGBR = {\n",
    "    'n_estimators': Integer(10, 500), 'learning_rate': Real(0.01, 0.5, prior='log-uniform'),\n",
    "    'max_depth': Integer(1, 10), 'subsample': Real(0.5, 0.9, prior='uniform'),\n",
    "    'colsample_bytree': Real(0.5, 1.0, prior='uniform'), \n",
    "    'reg_alpha': Real(0.1, 1.0, prior='log-uniform'), 'reg_lambda': Real(0.1, 10.0, prior='log-uniform')\n",
    "}\n",
    "parameter_RF = {\n",
    "    'n_estimators': Integer(10, 200), 'max_depth': Integer(1, 20),\n",
    "    'max_features': Categorical(['sqrt', 'log2', 1.0]), 'min_samples_leaf': Integer(1, 10),\n",
    "    'min_samples_split': Integer(2, 10)\n",
    "}\n",
    "parameter_CBR = {\n",
    "    'iterations': Integer(10, 500), 'learning_rate': Real(0.01, 0.5, prior='log-uniform'),\n",
    "    'depth': Integer(1, 16), 'l2_leaf_reg': Real(0.1, 10.0, prior='log-uniform'),\n",
    "    'subsample': Real(0.5, 0.9, prior='uniform'), 'rsm': Real(0.5, 1.0, prior='uniform')\n",
    "}\n",
    "parameter_LGBM = {\n",
    "    'n_estimators': Integer(10, 500), 'learning_rate': Real(0.01, 0.8, prior='log-uniform'),\n",
    "    'max_depth': Integer(1, 10), 'num_leaves': Integer(5, 50),\n",
    "    'subsample': Real(0.5, 0.9, prior='uniform'), 'colsample_bytree': Real(0.5, 1.0, prior='uniform'),\n",
    "    'reg_alpha': Real(0.1, 10.0, prior='log-uniform'), 'reg_lambda': Real(0.1, 10.0, prior='log-uniform')\n",
    "}\n",
    "parameter_GBRT = {\n",
    "    'n_estimators': Integer(10, 500), 'learning_rate': Real(0.01, 0.5, prior='log-uniform'),\n",
    "    'max_depth': Integer(1, 10), 'max_features': Categorical(['sqrt', 'log2', 1.0]),\n",
    "    'min_samples_split': Integer(2, 10), 'min_samples_leaf': Integer(1, 10),\n",
    "    'subsample': Real(0.5, 0.9, prior='uniform')\n",
    "}\n",
    "parameter_HGBR = {\n",
    "    'learning_rate': Real(0.01, 0.5, prior='log-uniform'), 'max_iter': Integer(10, 500),\n",
    "    'max_depth': Integer(1, 10), 'min_samples_leaf': Integer(2, 10),\n",
    "    'l2_regularization': Real(0.1, 10.0, prior='log-uniform')\n",
    "}\n",
    "parameter_ETR = {\n",
    "    'n_estimators': Integer(10, 200), 'max_depth': Integer(1, 10),\n",
    "    'min_samples_split': Integer(2, 10), 'min_samples_leaf': Integer(1, 10),\n",
    "    'max_features': Categorical(['sqrt', 'log2', 1.0])\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# --- MAIN EXECUTION PIPELINE ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Environment Initialization ---\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "print(\"--- Pipeline Initialized ---\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    lightgbm_available = False\n",
    "print(\"Dependencies verified.\")\n",
    "\n",
    "\n",
    "# --- 2. Data Ingestion and Preprocessing ---\n",
    "X, Y = pd.DataFrame(), pd.Series()\n",
    "try:\n",
    "    print(f\"\\nImporting dataset: {EXCEL_FILE_PATH}\")\n",
    "    df = pd.read_excel(EXCEL_FILE_PATH)\n",
    "    print(\"Dataset imported successfully.\")\n",
    "\n",
    "    min_cols_required = 2\n",
    "    if isinstance(X_COLS_SLICE, slice):\n",
    "        min_cols_required = max(min_cols_required, abs(X_COLS_SLICE.start or 0), abs(X_COLS_SLICE.stop or 0))\n",
    "    if df.shape[1] < min_cols_required:\n",
    "         raise ValueError(f\"CRITICAL: Insufficient column count for operation X_COLS_SLICE={X_COLS_SLICE}.\")\n",
    "\n",
    "    X = df.iloc[:, X_COLS_SLICE]\n",
    "    Y = df.iloc[:, Y_COLS_SLICE]\n",
    "    print(f\"Feature/Target extraction complete. X shape={X.shape}, Y shape={Y.shape} (Target: '{Y.name}')\")\n",
    "\n",
    "    print(\"\\nSanitizing feature column names for estimator compatibility...\")\n",
    "    X.columns = [re.sub(r'\\[|\\]|<', '_', col) for col in X.columns]\n",
    "    print(\"Column sanitization complete.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n!!! CRITICAL: File '{EXCEL_FILE_PATH}' not found. Check configuration.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\n!!! CRITICAL: Data ingestion failure: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Model Instantiation and Validation Strategy ---\n",
    "cross_Valid = KFold(n_splits=CV_N_SPLITS, shuffle=CV_SHUFFLE, random_state=CV_RANDOM_STATE)\n",
    "\n",
    "# Registry of all supported estimators\n",
    "ALL_POSSIBLE_ESTIMATORS = {\n",
    "    'XGBR': XGB.XGBRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE, objective='reg:squarederror'),\n",
    "    'RF': RandomForestRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'GBRT': GradientBoostingRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'HGBR': HistGradientBoostingRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'ETR': ExtraTreesRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE)\n",
    "}\n",
    "if catboost_available:\n",
    "    ALL_POSSIBLE_ESTIMATORS['CBR'] = CatBoostRegressor(verbose=False, random_state=DEFAULT_MODEL_RANDOM_STATE, allow_writing_files=False)\n",
    "else:\n",
    "    print(\"Info: CatBoost module not found; related configurations ignored.\")\n",
    "\n",
    "if lightgbm_available:\n",
    "    ALL_POSSIBLE_ESTIMATORS['LGBM'] = LGBMRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE, verbosity=-1, objective='regression')\n",
    "else:\n",
    "    print(\"Info: LightGBM module not found; related configurations ignored.\")\n",
    "\n",
    "# Filter active estimators based on user configuration\n",
    "estimators_for_bayes = {name: ALL_POSSIBLE_ESTIMATORS[name]\n",
    "                        for name in ENABLED_MODELS\n",
    "                        if name in ALL_POSSIBLE_ESTIMATORS}\n",
    "\n",
    "params_mapping = {\n",
    "    'XGBR': parameter_XGBR, 'RF': parameter_RF, 'GBRT': parameter_GBRT,\n",
    "    'HGBR': parameter_HGBR, 'ETR': parameter_ETR\n",
    "}\n",
    "if catboost_available: params_mapping['CBR'] = parameter_CBR\n",
    "if lightgbm_available: params_mapping['LGBM'] = parameter_LGBM\n",
    "\n",
    "if not estimators_for_bayes:\n",
    "    print(\"\\n!!! CRITICAL: No active models configured. Verify 'ENABLED_MODELS' in configuration.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nPipeline ready. Starting optimization ({cross_Valid.get_n_splits()}-fold CV) for models: {list(estimators_for_bayes.keys())}\")\n",
    "\n",
    "\n",
    "# --- 4. Bayesian Optimization Loop ---\n",
    "grid_searches = {}\n",
    "print(f\"\\nInitiating BayesSearchCV (Iterations={N_ITER_BAYESIAN})...\")\n",
    "\n",
    "for name, estimator in estimators_for_bayes.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n--- Optimizing {name} ---\")\n",
    "    if name not in params_mapping:\n",
    "        print(f\"Warning: No hyperparameter space defined for {name}. Skipping.\")\n",
    "        grid_searches[name] = None\n",
    "        continue\n",
    "\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=estimator, search_spaces=params_mapping[name],\n",
    "        n_iter=N_ITER_BAYESIAN, scoring='r2', cv=cross_Valid,\n",
    "        n_jobs=-1, random_state=DEFAULT_MODEL_RANDOM_STATE, verbose=1\n",
    "    )\n",
    "    try:\n",
    "        bayes_search.fit(X, Y)\n",
    "        duration = time.time() - start_time\n",
    "        grid_searches[name] = bayes_search\n",
    "        print(f\"--- {name} Optimization Complete ---\")\n",
    "        print(f\"  Best Score (CV R²): {bayes_search.best_score_:.4f}\")\n",
    "        print(f\"  Best Parameters: {dict(bayes_search.best_params_)}\")\n",
    "        print(f\"  Runtime: {duration:.2f} s\")\n",
    "    except Exception as e:\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"\\n!!! ERROR: Optimization failed for {name}: {e}\")\n",
    "        print(f\"  Runtime before failure: {duration:.2f} s\")\n",
    "        grid_searches[name] = None\n",
    "\n",
    "# ==============================================================================\n",
    "# --- 5. RESULTS SUMMARY ---\n",
    "# ==============================================================================\n",
    "print(\"\\n\\n==============================================================================\")\n",
    "print(\"--- OPTIMIZATION REPORT ---\")\n",
    "print(\"==============================================================================\")\n",
    "\n",
    "# Storage for optimal hyperparameters\n",
    "all_best_params_for_export = {}\n",
    "\n",
    "# Iterate and report results\n",
    "for name, search_result in grid_searches.items():\n",
    "    print(f\"\\n--- Model: {name} ---\")\n",
    "    if search_result:\n",
    "        best_score = search_result.best_score_\n",
    "        best_params = dict(search_result.best_params_)\n",
    "        all_best_params_for_export[name] = best_params\n",
    "\n",
    "        print(f\"  Best R² Score (CV): {best_score:.4f}\")\n",
    "        print(\"  Optimal Hyperparameters:\")\n",
    "        for param, value in best_params.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"    - {param}: {value:.6f}\")\n",
    "            else:\n",
    "                print(f\"    - {param}: {value}\")\n",
    "    else:\n",
    "        print(\"  Status: Failed or Skipped.\")\n",
    "print(\"\\n--- Pipeline Execution Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1845944-833e-4b4c-8423-08ef4aa1f203",
   "metadata": {},
   "source": [
    "Model stacking, robustness verification, SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da648ba3-8d3c-4e2b-ae9c-de503ce25fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- USER CONFIGURATION AREA ---\n",
    "# ==============================================================================\n",
    "# Please modify all adjustable parameters within this section.\n",
    "\n",
    "# --- 1. Evaluation Protocol Settings ---\n",
    "# Number of seeds for repeated external cross-validation (Total runs of the full Stacking evaluation).\n",
    "N_SEEDS_FOR_EVALUATION = 100  \n",
    "# Number of folds for external cross-validation (Used for generating OOF predictions and evaluating Stacking).\n",
    "N_SPLITS_OUTER_CV = 10\n",
    "# Intensity factor for data augmentation via Gaussian noise (Set to 0 to disable).\n",
    "NOISE_SCALE_FACTOR = 0\n",
    "\n",
    "# --- 2. Meta-Learner Tuning Configuration ---\n",
    "# Number of iterations for Bayesian optimization of the Meta-Learner (Executed only during the first seed/iteration).\n",
    "META_LEARNER_N_ITER_BAYESIAN = 50\n",
    "# Number of folds for the Meta-Learner's internal cross-validation.\n",
    "META_LEARNER_N_SPLITS_CV = 10\n",
    "# Number of repeats for the Meta-Learner's internal cross-validation.\n",
    "META_LEARNER_N_REPEATS_CV = 10\n",
    "\n",
    "# --- 3. Visualization and Export Settings ---\n",
    "# Filename for the final output Excel report.\n",
    "OUTPUT_EXCEL_FILENAME = 'SHAP_Analysis_Results.xlsx'\n",
    "# Method to calculate weights for Level-1 aggregation ('1/RMSE', 'uniform', or 'exponential').\n",
    "WEIGHTING_METHOD = '1/RMSE'\n",
    "# Number of top features to display in the feature importance bar chart.\n",
    "N_FEATURES_TO_PLOT = 30\n",
    "# Whether to generate SHAP beeswarm plots (This process may be time-consuming).\n",
    "PLOT_SHAP_SWARM_PLOT = True\n",
    "# Maximum number of samples for SHAP beeswarm plots to prevent memory overflow. Set to None for no limit.\n",
    "SHAP_SWARM_SAMPLES_LIMIT = 5000\n",
    "\n",
    "# --- 4. Base Learner Selection ---\n",
    "# Select the models to participate as Base Learners in Stacking.\n",
    "# [Uncomment] a line to [ENABLE] the model.\n",
    "# [Comment out] a line to [DISABLE] the model.\n",
    "ENABLED_BASE_LEARNERS = [\n",
    "    'XGBR',   # XGBoost Regressor\n",
    "    'RF',     # Random Forest Regressor\n",
    "    'GBRT',   # Gradient Boosting Regressor\n",
    "    'HGBR',   # Histogram-based Gradient Boosting Regressor\n",
    "    'ETR',    # Extra Trees Regressor\n",
    "    'CBR',    # CatBoost Regressor\n",
    "    'LGBM',   # LightGBM Regressor\n",
    "]\n",
    "\n",
    "# --- 5. Global Random State Setting ---\n",
    "# Applied to all models and cross-validation to ensure reproducibility.\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- MAIN SCRIPT EXECUTION (Do not modify this section) ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Library Imports and Environment Check ---\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import shap\n",
    "import itertools\n",
    "from math import comb\n",
    "import scipy.stats as st\n",
    "\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.base import clone\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"--- Script execution started ---\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    lightgbm_available = False\n",
    "print(\"Library imports and environment check completed.\")\n",
    "\n",
    "# --- 2. Core Variable Loading and Preparation ---\n",
    "# Ensure required variables from the hyperparameter search phase are available.\n",
    "if 'X' not in locals() or 'X' not in globals():\n",
    "    print(\"Error: Variable 'X' is undefined. Please run the hyperparameter search script first.\")\n",
    "    print(\"Warning: 'X' and 'Y' not found. Generating dummy data for demonstration.\")\n",
    "    from sklearn.datasets import make_regression\n",
    "    X_dummy, Y_dummy = make_regression(n_samples=100, n_features=10, n_informative=5, random_state=DEFAULT_MODEL_RANDOM_STATE)\n",
    "    X = pd.DataFrame(X_dummy, columns=[f'Feature_{i+1}' for i in range(10)])\n",
    "    Y = pd.Series(Y_dummy)\n",
    "    grid_searches = {model: None for model in ENABLED_BASE_LEARNERS}\n",
    "\n",
    "if 'Y' not in locals() or 'Y' not in globals():\n",
    "    print(\"Error: Variable 'Y' is undefined. Please run the hyperparameter search script first.\")\n",
    "    exit()\n",
    "if 'grid_searches' not in locals() or 'grid_searches' not in globals():\n",
    "    print(\"Error: Variable 'grid_searches' is undefined. Please run the hyperparameter search script first.\")\n",
    "    exit()\n",
    "\n",
    "filtered_grid_searches = {name: gs for name, gs in grid_searches.items() if name in ENABLED_BASE_LEARNERS}\n",
    "model_mapping = {\n",
    "    'XGBR': XGB.XGBRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE), 'RF': RandomForestRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'GBRT': GradientBoostingRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE), 'HGBR': HistGradientBoostingRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'ETR': ExtraTreesRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "    'CBR': CatBoostRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE, verbose=0) if catboost_available else None,\n",
    "    'LGBM': LGBMRegressor(random_state=DEFAULT_MODEL_RANDOM_STATE, verbosity=-1) if lightgbm_available else None,\n",
    "}\n",
    "for name, gs in filtered_grid_searches.items():\n",
    "    if gs is None:\n",
    "        print(f\"Warning: Optimization results for {name} not found. Using default model configuration.\")\n",
    "        class MockSearchResult:\n",
    "            def __init__(self, estimator): self.best_estimator_ = estimator\n",
    "        filtered_grid_searches[name] = MockSearchResult(model_mapping[name])\n",
    "\n",
    "model_names_available = list(filtered_grid_searches.keys())\n",
    "print(f\"\\nModel filtering completed: Selected {len(filtered_grid_searches)} enabled models: {model_names_available}\")\n",
    "if not filtered_grid_searches: print(\"\\n!!! FATAL ERROR: No valid Base Learners selected.\"); exit()\n",
    "feature_names_list = X.columns.tolist()\n",
    "\n",
    "# --- 3. Core Function Definitions ---\n",
    "def initialize_best_estimators(grid_searches_dict):\n",
    "    estimators_init = {}\n",
    "    print(\"Initializing models...\")\n",
    "    for name, search_result in grid_searches_dict.items():\n",
    "        if hasattr(search_result, 'best_estimator_'):\n",
    "            estimators_init[name] = clone(search_result.best_estimator_)\n",
    "            print(f\"  Successfully initialized {name}.\")\n",
    "        else: print(f\"  Warning: Optimization results for {name} not found. Skipping this model.\")\n",
    "    return estimators_init\n",
    "\n",
    "def calculate_weights(scores, method='1/RMSE'):\n",
    "    scores = np.array(scores)\n",
    "    scores[np.isinf(scores) | np.isnan(scores) | (scores < 1e-9)] = 1e-9\n",
    "    if method == 'uniform': weights = np.ones_like(scores)\n",
    "    elif method == 'exponential': weights = np.exp(-scores)\n",
    "    else: weights = 1.0 / scores\n",
    "    total_weight = np.sum(weights)\n",
    "    return weights / total_weight if total_weight > 1e-9 else np.ones_like(scores) / len(scores)\n",
    "\n",
    "# =================================================================================\n",
    "# --- 4. Optimization of Meta-Learner Hyperparameters (One-time Execution) ---\n",
    "# =================================================================================\n",
    "print(\"\\n--- Step 1: Optimization of Meta-Learner Hyperparameters ---\")\n",
    "print(\"  Generating global OOF predictions for Meta-Learner tuning...\")\n",
    "oof_preds_full = np.zeros((len(X), len(model_names_available)))\n",
    "kf_for_meta = KFold(n_splits=META_LEARNER_N_SPLITS_CV, shuffle=True, random_state=DEFAULT_MODEL_RANDOM_STATE)\n",
    "base_estimators_for_meta = initialize_best_estimators(filtered_grid_searches)\n",
    "\n",
    "for i, (name, estimator) in enumerate(base_estimators_for_meta.items()):\n",
    "    fold_preds = np.zeros(len(X))\n",
    "    for train_idx, val_idx in kf_for_meta.split(X):\n",
    "        X_train_fold, X_val_fold, y_train_fold = X.iloc[train_idx], X.iloc[val_idx], Y.iloc[train_idx]\n",
    "        est_clone = clone(estimator); est_clone.fit(X_train_fold, y_train_fold)\n",
    "        fold_preds[val_idx] = est_clone.predict(X_val_fold)\n",
    "    oof_preds_full[:, i] = fold_preds\n",
    "\n",
    "print(\"  Performing Bayesian optimization on global OOF predictions...\")\n",
    "meta_learner_params = {'elasticnet__alpha': Real(1e-5, 10.0, prior='log-uniform'), 'elasticnet__l1_ratio': Real(0.0, 1.0, prior='uniform')}\n",
    "meta_pipeline_template = Pipeline([('scaler', StandardScaler()), ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))])\n",
    "\n",
    "meta_bayes_search = BayesSearchCV(\n",
    "    estimator=meta_pipeline_template, \n",
    "    search_spaces=meta_learner_params, \n",
    "    n_iter=META_LEARNER_N_ITER_BAYESIAN, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    cv=KFold(n_splits=META_LEARNER_N_SPLITS_CV, shuffle=True, random_state=DEFAULT_MODEL_RANDOM_STATE), \n",
    "    n_jobs=-1, \n",
    "    random_state=DEFAULT_MODEL_RANDOM_STATE, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "    meta_bayes_search.fit(oof_preds_full, Y)\n",
    "\n",
    "# Store the optimal parameters for subsequent independent training.\n",
    "best_meta_learner_params = meta_bayes_search.best_params_\n",
    "print(f\"  Meta-Learner tuning completed. Best CV RMSE: {-meta_bayes_search.best_score_:.4f}\")\n",
    "print(f\"  Optimal parameters found: {best_meta_learner_params}\")\n",
    "print(f\"  These parameters will be used for independent training in each split.\")\n",
    "\n",
    "\n",
    "# --- 5. Main Execution Pipeline: Outer Cross-Validation (Unbiased Stacking Evaluation) ---\n",
    "outer_cv = KFold(n_splits=N_SPLITS_OUTER_CV, shuffle=True, random_state=DEFAULT_MODEL_RANDOM_STATE)\n",
    "print(f\"\\n--- Step 2: Initiating Outer Cross-Validation (Folds={N_SPLITS_OUTER_CV}) ---\")\n",
    "\n",
    "all_split_results, all_fold_scores = [], []\n",
    "base_estimators = initialize_best_estimators(filtered_grid_searches)\n",
    "split_counter = 0\n",
    "\n",
    "for train_idx, val_idx in outer_cv.split(X, Y):\n",
    "    split_counter += 1\n",
    "    print(f\"  --- Processing Fold {split_counter}/{N_SPLITS_OUTER_CV} ---\")\n",
    "    X_train, X_val, y_train, y_val = X.iloc[train_idx], X.iloc[val_idx], Y.iloc[train_idx], Y.iloc[val_idx]\n",
    "    \n",
    "    oof_preds_val = np.zeros((len(y_val), len(model_names_available)))\n",
    "    split_result = {'split_index': split_counter, 'base_model_scores': {}, 'base_model_shap': {}, 'X_val': X_val}\n",
    "    \n",
    "    # Train Base Models\n",
    "    for i, (name, estimator) in enumerate(base_estimators.items()):\n",
    "        est_clone = clone(estimator); est_clone.fit(X_train, y_train)\n",
    "        preds_on_val = est_clone.predict(X_val); oof_preds_val[:, i] = preds_on_val\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, preds_on_val)); mae = mean_absolute_error(y_val, preds_on_val)\n",
    "        split_result['base_model_scores'][name] = {'rmse': rmse, 'mae': mae}\n",
    "        print(f\"    - {name}: RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "        all_fold_scores.extend([{'Split': split_counter, 'Model': name, 'Metric': 'RMSE', 'Value': rmse}, \n",
    "                                {'Split': split_counter, 'Model': name, 'Metric': 'MAE', 'Value': mae}])\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(est_clone)\n",
    "            shap_values = explainer.shap_values(X_val) if name == 'CBR' else explainer(X_val).values\n",
    "            split_result['base_model_shap'][name] = {'shap_values': shap_values, 'expected_value': explainer.expected_value}\n",
    "        except Exception as e:\n",
    "            print(f\"      Warning: SHAP calculation failed for {name}: {e}\")\n",
    "\n",
    "    # --- Train Meta-Learner independently for current split (Prevent Data Leakage) ---\n",
    "    # 1. Generate OOF predictions internally within the current external training set (X_train).\n",
    "    oof_preds_train_for_meta = np.zeros((len(X_train), len(model_names_available)))\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=DEFAULT_MODEL_RANDOM_STATE) \n",
    "    for inner_train_idx, inner_val_idx in inner_cv.split(X_train):\n",
    "        X_inner_train, X_inner_val, y_inner_train = X_train.iloc[inner_train_idx], X_train.iloc[inner_val_idx], y_train.iloc[inner_train_idx]\n",
    "        for i, (name, estimator) in enumerate(base_estimators.items()):\n",
    "            est_inner_clone = clone(estimator); est_inner_clone.fit(X_inner_train, y_inner_train)\n",
    "            oof_preds_train_for_meta[inner_val_idx, i] = est_inner_clone.predict(X_inner_val)\n",
    "\n",
    "    # 2. Instantiate a new Meta-Learner using the optimal hyperparameters found.\n",
    "    meta_learner_for_split = clone(meta_pipeline_template)\n",
    "    meta_learner_for_split.set_params(**best_meta_learner_params)\n",
    "    meta_learner_for_split.fit(oof_preds_train_for_meta, y_train)\n",
    "    \n",
    "    # 3. Predict and evaluate using this unbiased Meta-Learner.\n",
    "    y_pred_stacking = meta_learner_for_split.predict(oof_preds_val)\n",
    "    stacking_rmse = np.sqrt(mean_squared_error(y_val, y_pred_stacking)); stacking_mae = mean_absolute_error(y_val, y_pred_stacking)\n",
    "    split_result['stacking_score'] = {'rmse': stacking_rmse, 'mae': stacking_mae}\n",
    "    print(f\"    - Stacking: RMSE={stacking_rmse:.4f}, MAE={stacking_mae:.4f}\")\n",
    "    all_fold_scores.extend([{'Split': split_counter, 'Model': 'Stacking', 'Metric': 'RMSE', 'Value': stacking_rmse},\n",
    "                            {'Split': split_counter, 'Model': 'Stacking', 'Metric': 'MAE', 'Value': stacking_mae}])\n",
    "\n",
    "    all_split_results.append(split_result)\n",
    "print(\"  All data splits processed.\")\n",
    "\n",
    "\n",
    "# --- 6. Result Aggregation and SHAP Analysis (Two-Level Weighting Logic) ---\n",
    "print(\"\\n--- Step 3: Aggregating Results and Performing SHAP Analysis ---\")\n",
    "all_model_names = model_names_available + ['Stacking']\n",
    "final_analysis = {name: {} for name in all_model_names}\n",
    "\n",
    "with pd.ExcelWriter(OUTPUT_EXCEL_FILENAME, engine='openpyxl') as writer:\n",
    "    for model_name in all_model_names:\n",
    "        print(f\"\\n--- Analyzing Model: {model_name} ---\")\n",
    "        all_shap_values_list, all_x_val_list = [], []\n",
    "        \n",
    "        if model_name == 'Stacking':\n",
    "            print(\"  Applying [Two-Level Weighting] logic...\")\n",
    "            level1_shap_matrices, level2_raw_weights = [], []\n",
    "            for res in all_split_results:\n",
    "                base_rmses = [res['base_model_scores'][bn]['rmse'] for bn in model_names_available]\n",
    "                level1_weights = calculate_weights(base_rmses, WEIGHTING_METHOD)\n",
    "                \n",
    "                # Fetch SHAP values from the first model to initialize shape\n",
    "                first_shap_data = res['base_model_shap'].get(model_names_available[0])\n",
    "                if first_shap_data is None: continue\n",
    "                \n",
    "                weighted_shap_for_split = np.zeros_like(first_shap_data['shap_values'])\n",
    "                \n",
    "                # Weighted Sum of Base Models (Level 1)\n",
    "                for i, bn in enumerate(model_names_available):\n",
    "                    if (shap_data := res['base_model_shap'].get(bn)):\n",
    "                        weighted_shap_for_split += shap_data['shap_values'] * level1_weights[i]\n",
    "                \n",
    "                level1_shap_matrices.append(weighted_shap_for_split)\n",
    "                all_x_val_list.append(res['X_val'])\n",
    "                level2_raw_weights.append(res['stacking_score']['rmse'])\n",
    "            \n",
    "            if not level1_shap_matrices:\n",
    "                print(f\"  Model {model_name} has no valid SHAP data, skipping analysis.\"); continue\n",
    "            \n",
    "            combined_shap_df = pd.DataFrame(np.vstack(level1_shap_matrices), columns=feature_names_list)\n",
    "            \n",
    "            # Weighted Average across Splits (Level 2)\n",
    "            level2_weights = calculate_weights(level2_raw_weights, method='1/RMSE')\n",
    "            expanded_sample_weights = np.repeat(level2_weights, [len(x) for x in all_x_val_list])\n",
    "            \n",
    "            global_importance = pd.Series(np.average(combined_shap_df.abs().values, axis=0, weights=expanded_sample_weights), index=feature_names_list).sort_values(ascending=False)\n",
    "        \n",
    "        else:\n",
    "            print(\"  Applying [Arithmetic Mean] logic...\")\n",
    "            for res in all_split_results:\n",
    "                if (shap_data := res['base_model_shap'].get(model_name)):\n",
    "                    all_shap_values_list.append(pd.DataFrame(shap_data['shap_values'], columns=feature_names_list))\n",
    "                    all_x_val_list.append(res['X_val'])\n",
    "            \n",
    "            if not all_shap_values_list:\n",
    "                print(f\"  Model {model_name} has no valid SHAP data, skipping analysis.\"); continue\n",
    "            \n",
    "            combined_shap_df = pd.concat(all_shap_values_list, ignore_index=True)\n",
    "            global_importance = pd.Series(np.mean(combined_shap_df.abs().values, axis=0), index=feature_names_list).sort_values(ascending=False)\n",
    "        \n",
    "        if 'combined_shap_df' not in locals() or combined_shap_df.empty: continue\n",
    "        combined_x_val_df = pd.concat(all_x_val_list, ignore_index=True)\n",
    "        final_analysis[model_name]['global_importance'] = global_importance\n",
    "        \n",
    "        # Export data\n",
    "        global_importance.to_excel(writer, sheet_name=f'{model_name}_GlobalImportance')\n",
    "        plot_df = pd.concat([combined_x_val_df.reset_index(drop=True), combined_shap_df.reset_index(drop=True).add_prefix('SHAP_')], axis=1)\n",
    "        plot_df = plot_df[list(global_importance.index) + [f'SHAP_{f}' for f in global_importance.index]]\n",
    "        plot_df.to_excel(writer, sheet_name=f'{model_name}_SwarmPlotData', index=False)\n",
    "        \n",
    "        # Visualization\n",
    "        print(\"  Generating SHAP Beeswarm Plots and Feature Importance Bar Charts...\")\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "        feature_order = global_importance.head(N_FEATURES_TO_PLOT).index.tolist()\n",
    "        \n",
    "        plt.sca(ax1)\n",
    "        shap.summary_plot(combined_shap_df[feature_order].values, combined_x_val_df[feature_order], feature_names=feature_order, show=False)\n",
    "        ax1.set_title(f'SHAP Beeswarm Plot')\n",
    "        \n",
    "        top_features_for_plot = global_importance.reindex(feature_order)\n",
    "        sns.barplot(x=top_features_for_plot.values, y=top_features_for_plot.index, ax=ax2, orient='h', palette='viridis')\n",
    "        ax2.set_title(f'Global Feature Importance')\n",
    "        ax2.set_xlabel(f'Mean(|SHAP Value|) - {\"Two-Level Weighted\" if model_name == \"Stacking\" else \"Arithmetic Mean\"}')\n",
    "        ax2.set_ylabel('')\n",
    "        \n",
    "        fig.suptitle(f'SHAP Analysis for {model_name} (CV Folds={N_SPLITS_OUTER_CV})', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    # --- 7. Performance Metrics Processing and Export ---\n",
    "    print(\"\\n--- Step 4: Processing and Exporting Performance Metrics ---\")\n",
    "    if all_fold_scores:\n",
    "        scores_df = pd.DataFrame(all_fold_scores)\n",
    "        try:\n",
    "            print(\"  Exporting per-fold performance metrics to Excel...\")\n",
    "            scores_pivot_df = scores_df.pivot_table(index='Split', columns=['Model', 'Metric'], values='Value')\n",
    "            scores_pivot_df.columns = [f'{col[1]}_{col[0]}' for col in scores_pivot_df.columns]\n",
    "            scores_pivot_df.to_excel(writer, sheet_name='Fold_Performance_Metrics')\n",
    "            print(f\"  Successfully exported to sheet 'Fold_Performance_Metrics'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  !!! Error exporting performance metrics: {e}\")\n",
    "\n",
    "        print(\"\\n--- Step 5: Final Average Performance (Arithmetic Mean) ---\")\n",
    "        mean_scores_summary = scores_pivot_df.mean().reset_index()\n",
    "        mean_scores_summary.columns = ['Metric_Model', 'Mean_Value']\n",
    "        mean_scores_summary[['Metric', 'Model']] = mean_scores_summary['Metric_Model'].str.split('_', expand=True)\n",
    "        final_summary_table = mean_scores_summary.pivot_table(index='Model', columns='Metric', values='Mean_Value')\n",
    "        if 'RMSE' in final_summary_table.columns and 'MAE' in final_summary_table.columns:\n",
    "            final_summary_table = final_summary_table[['RMSE', 'MAE']]\n",
    "        print(final_summary_table.to_string())\n",
    "    else:\n",
    "        print(\"  No performance metrics collected.\")\n",
    "\n",
    "    # --- 8. Global Feature Importance Summary Generation ---\n",
    "    print(\"\\n--- Step 6: Generating Global Feature Importance Summary for All Models ---\")\n",
    "    if (importance_series_list := [res['global_importance'].rename(name) for name, res in final_analysis.items() if 'global_importance' in res and not res['global_importance'].empty]):\n",
    "        summary_importance_df = pd.concat(importance_series_list, axis=1).sort_index()\n",
    "        print(\"\\nGlobal Feature Importance Summary:\")\n",
    "        print(summary_importance_df.to_string())\n",
    "        summary_importance_df.to_excel(writer, sheet_name='Global_Importance_Summary')\n",
    "    else:\n",
    "        print(\"No feature importance data collected.\")\n",
    "\n",
    "print(f\"\\n--- Analysis Completed. Results exported to {OUTPUT_EXCEL_FILENAME} ---\")\n",
    "print(f\"--- Script execution finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c3e97-a25f-4b59-aadb-325f93645e05",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a54db-fae5-4475-a880-2c0654c2d432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- USER CONFIGURATION AREA ---\n",
    "# ==============================================================================\n",
    "# Please modify all adjustable parameters within this section.\n",
    "\n",
    "# --- 1. Prediction Data Settings ---\n",
    "# Filename of the unknown dataset for prediction (Excel format).\n",
    "UNKNOWN_DATA_FILE = 'prediction-FGA-Eb.xlsx'\n",
    "# UNKNOWN_DATA_FILE = 'prediction-FGA-NVOA.xlsx'\n",
    "# Column slicing definition for feature extraction (X_new) from Excel.\n",
    "# (slice(None), slice(1, None)) selects all rows and columns starting from index 1.\n",
    "# This effectively skips the first column (index 0) of the Excel file.\n",
    "UNKNOWN_DATA_FILE_COLUMN_RANGE = (slice(None), slice(1, None)) \n",
    "\n",
    "# --- 2. Model Reuse and Training Configuration ---\n",
    "# Whether to attempt reusing the Stacking Model trained in the previous script.\n",
    "REUSE_PRETRAINED_STACKING_MODEL = False\n",
    "\n",
    "# The following parameters apply only when REUSE_PRETRAINED_STACKING_MODEL = False (Retraining mode).\n",
    "PREDICT_CV_N_SPLITS = 10                  # Number of folds for generating OOF predictions.\n",
    "PREDICT_META_LEARNER_N_SPLITS_CV = 10     # Number of folds for Meta-Learner internal cross-validation.\n",
    "PREDICT_META_LEARNER_N_REPEATS_CV = 10    # Number of repeats for Meta-Learner internal cross-validation.\n",
    "PREDICT_META_LEARNER_N_ITER_BAYESIAN = 50 # Number of Bayesian optimization iterations for the Meta-Learner.\n",
    "\n",
    "# --- 3. Base Learner Selection ---\n",
    "# !!! IMPORTANT: This list must match the configuration in 'optimize_base_learners.py' and 'evaluate_stacking_model.py' !!!\n",
    "ENABLED_BASE_LEARNERS = [\n",
    "    'XGBR',    # XGBoost Regressor\n",
    "    'RF',      # Random Forest Regressor\n",
    "    'GBRT',    # Gradient Boosting Regressor\n",
    "    'HGBR',    # Histogram-based Gradient Boosting Regressor\n",
    "    'ETR',     # Extra Trees Regressor\n",
    "    'CBR',     # CatBoost Regressor\n",
    "    'LGBM',    # LightGBM Regressor\n",
    "]\n",
    "\n",
    "# --- 4. Result Export Settings ---\n",
    "PREDICTION_OUTPUT_FILENAME_PREFIX = 'unknown_predictions'\n",
    "PREDICTION_EXPORT_TO_EXCEL = True\n",
    "PREDICTION_EXPORT_TO_CSV = False\n",
    "\n",
    "# --- 5. Global Random State Setting ---\n",
    "# !!! IMPORTANT: This value must be consistent with DEFAULT_MODEL_RANDOM_STATE in other scripts !!!\n",
    "DEFAULT_MODEL_RANDOM_STATE = 0 \n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- MAIN SCRIPT EXECUTION (Do not modify this section) ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. Library Imports and Environment Check ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import sys \n",
    "\n",
    "import xgboost as XGB\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.base import clone\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "\n",
    "print(\"--- Initiating Prediction on Unknown Data ---\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    catboost_available = True\n",
    "except ImportError:\n",
    "    catboost_available = False\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    lightgbm_available = True\n",
    "except ImportError:\n",
    "    lightgbm_available = False\n",
    "print(\"Library imports and environment check completed.\")\n",
    "\n",
    "\n",
    "# --- 2. Core Variable Loading and Preparation ---\n",
    "# Check for the existence of required variables from upstream scripts.\n",
    "if 'X' not in locals() and 'X' not in globals():\n",
    "    print(\"Error: Variable 'X' (Training Features) is undefined. Please ensure the data preparation and optimization scripts have been executed.\")\n",
    "    sys.exit(1)\n",
    "if 'Y' not in locals() and 'Y' not in globals():\n",
    "    print(\"Error: Variable 'Y' (Training Targets) is undefined. Please ensure the data preparation and optimization scripts have been executed.\")\n",
    "    sys.exit(1)\n",
    "if 'grid_searches' not in locals() and 'grid_searches' not in globals():\n",
    "    print(\"Error: Variable 'grid_searches' is undefined. Please ensure the optimization script has been executed.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Filter the optimized models based on the current configuration.\n",
    "filtered_grid_searches = {name: gs for name, gs in grid_searches.items() if name in ENABLED_BASE_LEARNERS}\n",
    "print(f\"\\nModel filtering completed: Selected {len(filtered_grid_searches)} enabled models from {len(grid_searches)} available: {list(filtered_grid_searches.keys())}\")\n",
    "if not filtered_grid_searches:\n",
    "    print(\"\\n!!! FATAL ERROR: No Base Learners selected. Please check the ENABLED_BASE_LEARNERS list in the configuration.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Sanitize feature names in the training set to prevent errors in models like XGBoost.\n",
    "# Store the sanitized list to ensure alignment with the prediction dataset.\n",
    "feature_names_list = list(X.columns)\n",
    "X.columns = [re.sub(r'\\[|\\]|<', '_', col) for col in feature_names_list]\n",
    "feature_names_list = list(X.columns) # Update the list to reflect sanitized names.\n",
    "\n",
    "print(f\"\\nAttempting to load unknown prediction dataset from '{UNKNOWN_DATA_FILE}'...\")\n",
    "if not os.path.exists(UNKNOWN_DATA_FILE):\n",
    "    print(f\"Error: Prediction file '{UNKNOWN_DATA_FILE}' does not exist. Please verify the file path.\")\n",
    "    sys.exit(1) \n",
    "else:\n",
    "    try:\n",
    "        full_data_from_excel = pd.read_excel(UNKNOWN_DATA_FILE)\n",
    "        # Apply slicing based on UNKNOWN_DATA_FILE_COLUMN_RANGE\n",
    "        X_new = full_data_from_excel.iloc[UNKNOWN_DATA_FILE_COLUMN_RANGE].copy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to load or process '{UNKNOWN_DATA_FILE}': {e}. Please check file content and column range settings.\")\n",
    "        sys.exit(1) \n",
    "\n",
    "original_X_new_index = X_new.index # Preserve original indices for result export.\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# --- Column Alignment Logic: Positional Mapping ---\n",
    "# --------------------------------------------------------------------------------\n",
    "# Verify that the number of features in the prediction set matches the training set.\n",
    "if X_new.shape[1] != len(feature_names_list):\n",
    "    print(f\"Error: Prediction set X_new has {X_new.shape[1]} columns, but training set has {len(feature_names_list)} columns. Feature count mismatch.\")\n",
    "    print(\"Please verify 'UNKNOWN_DATA_FILE_COLUMN_RANGE' to ensure the extracted feature count matches the training data.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Directly map training feature names to the prediction dataset columns by position.\n",
    "# This assumes the column order in the new dataset exactly matches the training data.\n",
    "# This prevents issues arising from minor naming discrepancies.\n",
    "X_new.columns = feature_names_list\n",
    "\n",
    "# Check for missing values in the raw prediction data.\n",
    "if X_new.isnull().any().any():\n",
    "    missing_cols_in_raw_X_new = X_new.columns[X_new.isnull().any()].tolist()\n",
    "    print(f\"Warning: Prediction dataset X_new contains missing values (NaN). Affected columns: {missing_cols_in_raw_X_new}\")\n",
    "    # X_new = X_new.fillna(0) # Optional: Uncomment to enable zero-filling if required.\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "print(f\"\\nTraining Data X shape: {X.shape}\")\n",
    "print(f\"Prediction Data X_new shape (after column alignment): {X_new.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. Core Functions and Model Training ---\n",
    "def initialize_best_estimators(grid_searches_dict):\n",
    "    estimators_init = {}\n",
    "    available_models = {\n",
    "        'XGBR': (XGB.XGBRegressor, {'objective': 'reg:squarederror', 'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'RF': (RandomForestRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'GBRT': (GradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'ETR': (ExtraTreesRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE}),\n",
    "        'HGBR': (HistGradientBoostingRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE})\n",
    "    }\n",
    "    if catboost_available: available_models['CBR'] = (CatBoostRegressor, {'verbose': False, 'random_state': DEFAULT_MODEL_RANDOM_STATE, 'allow_writing_files': False})\n",
    "    if lightgbm_available: available_models['LGBM'] = (LGBMRegressor, {'random_state': DEFAULT_MODEL_RANDOM_STATE, 'verbosity': -1, 'objective': 'regression'})\n",
    "\n",
    "    print(\"Initializing Base Learners...\")\n",
    "    for name in grid_searches_dict.keys():\n",
    "        if name not in available_models:\n",
    "            print(f\"  Warning: Model {name} is not in the available models list, skipping initialization.\")\n",
    "            continue\n",
    "        model_class, fixed_params = available_models[name]\n",
    "        if name in grid_searches_dict and grid_searches_dict[name] is not None and hasattr(grid_searches_dict[name], 'best_estimator_'):\n",
    "            try:\n",
    "                estimators_init[name] = grid_searches_dict[name].best_estimator_\n",
    "                print(f\"  Successfully initialized {name} using optimized parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (from best_estimator_): {e}. Skipping this model.\")\n",
    "        else:\n",
    "            print(f\"  Warning: Optimization results for {name} not found. Attempting initialization with default parameters.\")\n",
    "            try:\n",
    "                estimators_init[name] = model_class(**fixed_params)\n",
    "                print(f\"  Successfully initialized {name} using default parameters.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error initializing {name} (using defaults): {e}. Skipping this model.\")\n",
    "    initialized_estimators = {k: v for k, v in estimators_init.items() if v}\n",
    "    if not initialized_estimators: print(\"Warning: No models were successfully initialized!\")\n",
    "    return initialized_estimators\n",
    "\n",
    "meta_learner_params = {\n",
    "    'elasticnet__alpha': Real(1e-5, 10.0, prior='log-uniform', name='elasticnet__alpha'),\n",
    "    'elasticnet__l1_ratio': Real(0.0, 1.0, prior='uniform', name='elasticnet__l1_ratio')\n",
    "}\n",
    "final_meta_learner = None; base_estimators_for_final_training = None\n",
    "\n",
    "if REUSE_PRETRAINED_STACKING_MODEL:\n",
    "    print(\"\\n--- Attempting to reuse pre-trained Stacking Model ---\")\n",
    "    if 'global_best_meta_learner' in locals() and global_best_meta_learner is not None and hasattr(global_best_meta_learner, 'predict'):\n",
    "        final_meta_learner = global_best_meta_learner\n",
    "        print(\"Successfully reused the pre-trained Meta-Learner.\")\n",
    "    else:\n",
    "        print(\"Warning: Valid 'global_best_meta_learner' not found. Fallback to retraining mode.\")\n",
    "        REUSE_PRETRAINED_STACKING_MODEL = False\n",
    "\n",
    "if not REUSE_PRETRAINED_STACKING_MODEL:\n",
    "    print(\"\\n--- Retraining the final Stacked Model ---\")\n",
    "    base_estimators_for_oof = initialize_best_estimators(filtered_grid_searches)\n",
    "    if not base_estimators_for_oof:\n",
    "        print(\"Error: No Base Learners available for Stacking Model training.\"); sys.exit(1)\n",
    "\n",
    "    model_names_available = list(base_estimators_for_oof.keys())\n",
    "    oof_preds_for_meta_training = np.zeros((len(X), len(model_names_available)))\n",
    "    \n",
    "    kf_for_oof = KFold(n_splits=PREDICT_CV_N_SPLITS, shuffle=True, random_state=DEFAULT_MODEL_RANDOM_STATE)\n",
    "\n",
    "    print(f\"Starting {PREDICT_CV_N_SPLITS}-fold Cross-Validation to generate OOF predictions (Random Seed: {DEFAULT_MODEL_RANDOM_STATE})...\")\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(kf_for_oof.split(X, Y)):\n",
    "        print(f\"  Fold {fold_idx+1}/{PREDICT_CV_N_SPLITS}\")\n",
    "        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        Y_train_fold = Y.iloc[train_idx]\n",
    "        for i, (name, estimator_template) in enumerate(base_estimators_for_oof.items()):\n",
    "            estimator_fold = clone(estimator_template)\n",
    "            try:\n",
    "                estimator_fold.fit(X_train_fold, Y_train_fold)\n",
    "                oof_preds_for_meta_training[val_idx, i] = estimator_fold.predict(X_val_fold)\n",
    "            except Exception as e:\n",
    "                print(f\"    Warning: Model {name} failed during training or prediction: {e}. OOF predictions will remain zero.\")\n",
    "\n",
    "    if np.all(oof_preds_for_meta_training == 0) or np.any(np.isnan(oof_preds_for_meta_training)):\n",
    "        print(\"Error: Invalid OOF predictions; unable to train Meta-Learner. Please verify Base Learner training.\"); sys.exit(1)\n",
    "\n",
    "    print(\"Initiating Bayesian Optimization and training for Meta-Learner...\")\n",
    "    meta_bayes_search = BayesSearchCV(\n",
    "        estimator=Pipeline([('scaler', StandardScaler()), ('elasticnet', ElasticNet(random_state=DEFAULT_MODEL_RANDOM_STATE, max_iter=2000))]),\n",
    "        search_spaces=meta_learner_params, n_iter=PREDICT_META_LEARNER_N_ITER_BAYESIAN, scoring='r2',\n",
    "        cv=RepeatedKFold(n_splits=PREDICT_META_LEARNER_N_SPLITS_CV, n_repeats=PREDICT_META_LEARNER_N_REPEATS_CV, random_state=DEFAULT_MODEL_RANDOM_STATE),\n",
    "        n_jobs=-1, random_state=DEFAULT_MODEL_RANDOM_STATE, verbose=1)\n",
    "    try:\n",
    "        meta_bayes_search.fit(oof_preds_for_meta_training, Y)\n",
    "        final_meta_learner = meta_bayes_search.best_estimator_\n",
    "        print(\"\\n--- Meta-Learner Training Completed ---\")\n",
    "        print(f\"Meta-Learner Best Score (CV R² on Full OOF): {meta_bayes_search.best_score_:.4f}\")\n",
    "        print(f\"Meta-Learner Best Parameters: {dict(meta_bayes_search.best_params_)}\")\n",
    "\n",
    "        # Output coefficients of Base Learners within the Meta-Learner\n",
    "        if final_meta_learner and 'elasticnet' in final_meta_learner.named_steps:\n",
    "            elastic_net_model = final_meta_learner.named_steps['elasticnet']\n",
    "            if hasattr(elastic_net_model, 'coef_'):\n",
    "                print(\"\\n--- Base Learner Coefficients in Meta-Learner (ElasticNet) ---\")\n",
    "                for i, name in enumerate(model_names_available):\n",
    "                    if i < len(elastic_net_model.coef_):\n",
    "                        print(f\"  {name}: {elastic_net_model.coef_[i]:.6f}\")\n",
    "                    else:\n",
    "                        print(f\"  Warning: Coefficient for {name} not found (index out of bounds).\")\n",
    "            else:\n",
    "                print(\"Warning: Meta-Learner (ElasticNet) lacks 'coef_' attribute; cannot display coefficients.\")\n",
    "        else:\n",
    "            print(\"Warning: Meta-Learner or ElasticNet component not found; cannot display coefficients.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Meta-Learner training failed: {e}. Prediction cannot proceed.\"); sys.exit(1)\n",
    "\n",
    "print(\"\\n--- Retraining Final Base Learners on Full Dataset ---\")\n",
    "base_estimators_for_final_training = initialize_best_estimators(filtered_grid_searches)\n",
    "for name, estimator in base_estimators_for_final_training.items():\n",
    "    print(f\"  Training final {name} model...\")\n",
    "    try:\n",
    "        estimator.fit(X, Y)\n",
    "    except Exception as e:\n",
    "        print(f\"    Warning: Failed to train final {name} model: {e}. It will be removed from the prediction pipeline.\")\n",
    "        base_estimators_for_final_training[name] = None\n",
    "base_estimators_for_final_training = {k: v for k, v in base_estimators_for_final_training.items() if v}\n",
    "\n",
    "\n",
    "# --- 4. Prediction on Unknown Data ---\n",
    "if not final_meta_learner or not base_estimators_for_final_training:\n",
    "    print(\"FATAL ERROR: Stacking Model construction failed. Unable to proceed with prediction.\"); sys.exit(1)\n",
    "\n",
    "print(\"\\n--- Performing Stacked Prediction on Unknown Data ---\")\n",
    "base_predictions_on_new_data, base_model_names_for_prediction = [], []\n",
    "print(\"Generating Base Learner predictions on unknown data...\")\n",
    "for name, estimator in base_estimators_for_final_training.items():\n",
    "    try:\n",
    "        pred = estimator.predict(X_new)\n",
    "        base_predictions_on_new_data.append(pred)\n",
    "        base_model_names_for_prediction.append(name)\n",
    "        print(f\"  {name} prediction completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: Model {name} failed during prediction on unknown data: {e}. Skipping this model.\")\n",
    "\n",
    "if not base_predictions_on_new_data:\n",
    "    print(\"Error: All Base Learners failed to predict on unknown data.\"); sys.exit(1)\n",
    "\n",
    "meta_features_for_prediction = np.column_stack(base_predictions_on_new_data)\n",
    "print(f\"Base Learner Prediction Shape (Meta-Learner Input): {meta_features_for_prediction.shape}\")\n",
    "\n",
    "print(\"Executing final Stacked Prediction using Meta-Learner...\")\n",
    "try:\n",
    "    final_stacked_predictions = final_meta_learner.predict(meta_features_for_prediction)\n",
    "    print(\"Final Stacked Prediction completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during Meta-Learner final prediction: {e}\"); sys.exit(1)\n",
    "\n",
    "\n",
    "# --- 5. Result Compilation and Export ---\n",
    "print(\"\\n--- Compiling and Exporting Prediction Results ---\")\n",
    "results_df = pd.DataFrame(index=original_X_new_index) # Use original indices\n",
    "for i, name in enumerate(base_model_names_for_prediction):\n",
    "    results_df[f'{name}_Prediction'] = base_predictions_on_new_data[i]\n",
    "results_df['Stacked_Prediction'] = final_stacked_predictions\n",
    "\n",
    "current_timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename_base = f\"{PREDICTION_OUTPUT_FILENAME_PREFIX}_{current_timestamp}\"\n",
    "\n",
    "if PREDICTION_EXPORT_TO_EXCEL:\n",
    "    excel_filename = f\"{output_filename_base}.xlsx\"\n",
    "    try:\n",
    "        results_df.to_excel(excel_filename, index=True, engine='openpyxl')\n",
    "        print(f\"Prediction results successfully exported to Excel: {excel_filename}\")\n",
    "    except ImportError:\n",
    "        print(\"!!! 'openpyxl' library required for Excel export. Falling back to CSV export.\")\n",
    "        PREDICTION_EXPORT_TO_EXCEL = False\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error exporting results to Excel: {e}\")\n",
    "        PREDICTION_EXPORT_TO_EXCEL = False\n",
    "\n",
    "if PREDICTION_EXPORT_TO_CSV or not PREDICTION_EXPORT_TO_EXCEL:\n",
    "    csv_filename = f\"{output_filename_base}.csv\"\n",
    "    try:\n",
    "        results_df.to_csv(csv_filename, index=True)\n",
    "        print(f\"Prediction results successfully exported to CSV: {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! Error exporting results to CSV: {e}\")\n",
    "\n",
    "print(\"\\nUnknown Data Prediction Script Execution Finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (matsci-ai)",
   "language": "python",
   "name": "matsci-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
