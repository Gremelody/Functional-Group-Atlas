{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cda2588-47fb-42f1-bca3-325b36a7aabb",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc98cfb-0a09-4efc-9e9e-e94e5b006b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- CONFIGURATION AREA (Modify all adjustable parameters here) ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. File Path Configuration ---\n",
    "INPUT_FILE = 'Original database-FGA.xlsx'  # Input Excel filename\n",
    "OUTPUT_DIR = 'feature_engineering_output' # Directory for all output results\n",
    "\n",
    "# --- 2. Core Algorithm Parameters ---\n",
    "RANDOM_STATE = 0  # Random seed for reproducibility (generally does not need modification)\n",
    "PEARSON_CORR_THRESHOLD = 0.8  # Pearson correlation threshold for Stage 1. Features exceeding this will be processed.\n",
    "\n",
    "# --- 2.1 Filter Method Selection Criterion ---\n",
    "# In Stage 1, when two features are highly correlated, one must be eliminated.\n",
    "# This parameter determines which one to retain.\n",
    "# Options:\n",
    "#   'mutual_info': Retain feature with higher [Mutual Information] with target (Recommended, captures non-linear relationships).\n",
    "#   'pearson':     Retain feature with higher [Pearson Correlation] with target (Captures linear relationships only).\n",
    "FILTER_METHOD_CRITERION = 'mutual_info'\n",
    "\n",
    "# --- 3. RF (Random Forest) Model Hyperparameters ---\n",
    "# Random Forest provides high robustness and resistance to overfitting, making it a stable choice for feature selection.\n",
    "RF_N_ESTIMATORS = 500        # Number of trees. 500 is generally stable for various sample sizes.\n",
    "RF_MAX_DEPTH = 4             # **Critical Parameter**: Strictly limit tree depth to prevent overfitting.\n",
    "RF_MIN_SAMPLES_SPLIT = 8     # Minimum samples required to split an internal node (approx. 10%), avoids learning noise.\n",
    "RF_MIN_SAMPLES_LEAF = 4      # Minimum samples required at a leaf node, ensuring predictions are based on a group of samples.\n",
    "RF_MAX_FEATURES = 'sqrt'     # Number of features to consider when looking for the best split (square root), increases diversity.\n",
    "\n",
    "# --- 4. SHAP Calculation & Iterative Screening Parameters ---\n",
    "PERFORMANCE_METRIC = 'r2'  # Metric used during iterative screening. Options: 'mae' (Lower is better) or 'r2' (Higher is better).\n",
    "SHAP_COARSE_SELECTION_PERCENT = 0.8  # Percentage of features to retain in SHAP coarse selection Stage. 0.8 means retaining top 80%.\n",
    "EARLY_STOPPING_PATIENCE = 50 # Early stopping \"patience\". Stop iteration if performance does not improve for this many consecutive rounds.\n",
    "\n",
    "# --- 5. Cross-Validation Parameters ---\n",
    "KFOLD_SPLITS = 10 # Number of folds (k) for K-Fold Cross-Validation. Common values are 5 or 10.\n",
    "\n",
    "# --- 6. Dataset Slicing Parameters ---\n",
    "# FEATURE_COLUMN_SLICE: String defining the slice of feature columns. '1:-1' means from 2nd column to the second-to-last column.\n",
    "FEATURE_COLUMN_SLICE = '1:-2'\n",
    "# TARGET_COLUMN_INDEX: Integer index of the target column. -1 represents the last column.\n",
    "TARGET_COLUMN_INDEX = -2\n",
    "# TARGET_COLUMN_INDEX = -1\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- MAIN CODE BODY (Do not modify the following section) ---\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import textwrap\n",
    "\n",
    "def load_data_from_excel():\n",
    "    \"\"\"\n",
    "    Load data from the Excel file, extract features (X) and target (Y) based on configuration,\n",
    "    and perform data cleaning.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from '{INPUT_FILE}'...\")\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: File '{INPUT_FILE}' does not exist.\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    df_original = df.copy()\n",
    "\n",
    "    try:\n",
    "        all_col_names = df.columns\n",
    "        \n",
    "        try:\n",
    "            slice_parts = FEATURE_COLUMN_SLICE.split(':')\n",
    "            start = int(slice_parts[0]) if slice_parts[0] else None\n",
    "            end = int(slice_parts[1]) if len(slice_parts) > 1 and slice_parts[1] else None\n",
    "            feature_slice = slice(start, end)\n",
    "            \n",
    "            feature_col_names = all_col_names[feature_slice]\n",
    "            target_col_name = all_col_names[TARGET_COLUMN_INDEX]\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Error: Invalid dataset slicing parameters '{FEATURE_COLUMN_SLICE}' or '{TARGET_COLUMN_INDEX}'. Please check configuration. Error details: {e}\")\n",
    "            exit()\n",
    "\n",
    "        if len(feature_col_names) > 1:\n",
    "            feature_range_str = f\"From column '{feature_col_names[0]}' to '{feature_col_names[-1]}'\"\n",
    "        elif len(feature_col_names) == 1:\n",
    "            feature_range_str = f\"Column '{feature_col_names[0]}'\"\n",
    "        else:\n",
    "            feature_range_str = \"No features\"\n",
    "\n",
    "        print(f\"Extracting features ({feature_range_str}) and target '{target_col_name}'.\")\n",
    "\n",
    "        if target_col_name in feature_col_names:\n",
    "            print(f\"Critical Error: Target column '{target_col_name}' is also identified as a feature column. Check Excel column order or slicing parameters.\")\n",
    "            exit()\n",
    "\n",
    "        X = df[feature_col_names]\n",
    "        Y = df[target_col_name]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unknown error occurred during column extraction: {e}.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Features X extracted, shape: {X.shape}\")\n",
    "    print(f\"Target Y '{Y.name}' extracted, shape: {Y.shape}\")\n",
    "\n",
    "    combined_df = pd.concat([X, Y], axis=1)\n",
    "    for col in combined_df.columns:\n",
    "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "\n",
    "    original_rows = len(combined_df)\n",
    "    combined_df_cleaned = combined_df.dropna()\n",
    "\n",
    "    if len(combined_df_cleaned) < original_rows:\n",
    "        print(f\"Warning: Removed {original_rows - len(combined_df_cleaned)} rows containing missing values.\")\n",
    "        df_original = df_original.loc[combined_df_cleaned.index]\n",
    "\n",
    "    X_cleaned = combined_df_cleaned.drop(columns=[Y.name])\n",
    "    y_cleaned = combined_df_cleaned[Y.name]\n",
    "\n",
    "    print(f\"Data loading complete. X shape: {X_cleaned.shape}, y shape: {y_cleaned.shape}\")\n",
    "    return X_cleaned, y_cleaned, df_original\n",
    "\n",
    "def step1_filter_high_correlated_features(X, y):\n",
    "    \"\"\"\n",
    "    Stage 1: Remove highly correlated features using filter methods.\n",
    "    The selection criterion is determined by the FILTER_METHOD_CRITERION parameter.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Stage 1: Filter High-Correlation Features ---\")\n",
    "\n",
    "    if X.shape[1] <= 1:\n",
    "        print(\"Insufficient feature count (<=1), skipping correlation filtering.\")\n",
    "        return X.copy(), []\n",
    "    if X.shape[0] <= 1:\n",
    "        print(\"Insufficient sample size (<=1), skipping correlation filtering.\")\n",
    "        return X.copy(), []\n",
    "\n",
    "    corr_matrix = X.corr(method='pearson').abs()\n",
    "    \n",
    "    # --- Select filtering logic based on parameter ---\n",
    "    if FILTER_METHOD_CRITERION == 'mutual_info':\n",
    "        print(\"Criterion: Retain feature with higher [Mutual Information] with target in correlated pairs.\")\n",
    "        mi_scores = mutual_info_regression(X, y, random_state=RANDOM_STATE)\n",
    "        feature_y_importance = pd.Series(mi_scores, index=X.columns)\n",
    "        sorted_features = feature_y_importance.sort_values(ascending=False).index.tolist()\n",
    "        criterion_name = \"Mutual Information\"\n",
    "    elif FILTER_METHOD_CRITERION == 'pearson':\n",
    "        print(\"Criterion: Retain feature with higher [Pearson Correlation] with target in correlated pairs.\")\n",
    "        feature_y_corr = {col: abs(X[col].corr(y)) for col in X.columns}\n",
    "        feature_y_importance = pd.Series(feature_y_corr)\n",
    "        sorted_features = sorted(X.columns, key=lambda col: feature_y_importance.get(col, -1), reverse=True)\n",
    "        criterion_name = \"Pearson Correlation\"\n",
    "    else:\n",
    "        print(f\"Error: Invalid criterion '{FILTER_METHOD_CRITERION}'. Please select 'pearson' or 'mutual_info' in the configuration area.\")\n",
    "        exit()\n",
    "\n",
    "    kept_features_final = []\n",
    "    all_dropped_features_set = set()\n",
    "    retained_to_dropped_map = defaultdict(list)\n",
    "\n",
    "    for current_feature in sorted_features:\n",
    "        if current_feature in all_dropped_features_set:\n",
    "            continue\n",
    "        kept_features_final.append(current_feature)\n",
    "        for other_feature in sorted_features:\n",
    "            if other_feature == current_feature or other_feature in all_dropped_features_set:\n",
    "                continue\n",
    "            if corr_matrix.loc[current_feature, other_feature] > PEARSON_CORR_THRESHOLD:\n",
    "                all_dropped_features_set.add(other_feature)\n",
    "                retained_to_dropped_map[current_feature].append(other_feature)\n",
    "\n",
    "    to_drop_list = list(all_dropped_features_set)\n",
    "\n",
    "    if to_drop_list:\n",
    "        print(f\"Based on Pearson correlation (>{PEARSON_CORR_THRESHOLD}) and {criterion_name} with target, the following features are removed:\")\n",
    "        output_data = []\n",
    "        for kept_feat, dropped_list in retained_to_dropped_map.items():\n",
    "            if dropped_list:\n",
    "                output_data.append({\n",
    "                    'kept_feature': kept_feat,\n",
    "                    'dropped_features': \", \".join(sorted(dropped_list))\n",
    "                })\n",
    "        if output_data:\n",
    "            col1_header = f'Retained Feature (Higher {criterion_name})'\n",
    "            max_kept_len = max(len(row['kept_feature']) for row in output_data)\n",
    "            col1_width = max(max_kept_len, len(col1_header)) + 4\n",
    "            col2_header = 'Eliminated Features (High correlation with retained)'\n",
    "            terminal_width = 120\n",
    "            col2_width = terminal_width - col1_width\n",
    "            print(f\"\\n{col1_header:<{col1_width}}{col2_header}\")\n",
    "            print(f\"{'-' * (col1_width - 1)} {'-' * len(col2_header)}\")\n",
    "            for row in output_data:\n",
    "                kept_feat = row['kept_feature']\n",
    "                dropped_feats_str = row['dropped_features']\n",
    "                wrapped_lines = textwrap.wrap(dropped_feats_str, width=col2_width)\n",
    "                print(f\"{kept_feat:<{col1_width}}{wrapped_lines[0] if wrapped_lines else ''}\")\n",
    "                for i in range(1, len(wrapped_lines)):\n",
    "                    print(f\"{'':<{col1_width}}{wrapped_lines[i]}\")\n",
    "        X_filtered = X.drop(columns=to_drop_list)\n",
    "    else:\n",
    "        print(\"No feature pairs exceeded the Pearson correlation threshold. No features removed.\")\n",
    "        X_filtered = X.copy()\n",
    "\n",
    "    print(f\"\\nFeature count after filtering: {X_filtered.shape[1]}\")\n",
    "    return X_filtered, to_drop_list\n",
    "\n",
    "\n",
    "def step2_embed_shap_coarse_selection(X_filtered, y):\n",
    "    \"\"\"\n",
    "    Stage 2: Coarse feature selection based on SHAP values (Embedded method).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Stage 2: SHAP Coarse Selection ({KFOLD_SPLITS}-Fold CV) ---\")\n",
    "\n",
    "    if X_filtered.shape[1] == 0:\n",
    "        print(\"No features available for SHAP coarse selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "    if X_filtered.shape[0] < KFOLD_SPLITS:\n",
    "        print(f\"Sample size ({X_filtered.shape[0]}) is less than K-folds ({KFOLD_SPLITS}). Cannot perform Cross-Validation.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    avg_abs_shap_values_per_fold = []\n",
    "\n",
    "    print(f\"Calculating SHAP values (Using {KFOLD_SPLITS}-Fold CV)...\")\n",
    "    \n",
    "    # Check if sample size meets model minimum requirements\n",
    "    train_size_per_fold = X_filtered.shape[0] * (KFOLD_SPLITS - 1) // KFOLD_SPLITS\n",
    "    min_samples_for_model = max(RF_MIN_SAMPLES_LEAF, RF_MIN_SAMPLES_SPLIT)\n",
    "    if train_size_per_fold < min_samples_for_model:\n",
    "        print(f\"Warning: Training set size per fold ({train_size_per_fold}) is smaller than RF model minimum requirement ({min_samples_for_model}). Skipping SHAP coarse selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    for train_idx, val_idx in tqdm(kf.split(X_filtered), total=KFOLD_SPLITS, desc=\"SHAP Coarse Selection Progress\"):\n",
    "        X_fold_train, y_fold_train = X_filtered.iloc[train_idx], y.iloc[train_idx]\n",
    "        scaler = StandardScaler().fit(X_fold_train)\n",
    "        X_fold_train_scaled = scaler.transform(X_fold_train)\n",
    "        \n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=RF_N_ESTIMATORS,\n",
    "            max_depth=RF_MAX_DEPTH,\n",
    "            min_samples_split=RF_MIN_SAMPLES_SPLIT,\n",
    "            min_samples_leaf=RF_MIN_SAMPLES_LEAF,\n",
    "            max_features=RF_MAX_FEATURES,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        try:\n",
    "            rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "            explainer = shap.TreeExplainer(rf_model)\n",
    "            shap_values_fold = explainer.shap_values(X_fold_train_scaled)\n",
    "            if shap_values_fold.ndim == 1:\n",
    "                avg_abs_shap_values_per_fold.append(np.abs(shap_values_fold))\n",
    "            else:\n",
    "                avg_abs_shap_values_per_fold.append(np.abs(shap_values_fold).mean(axis=0))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Model training or SHAP calculation failed during coarse selection: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not avg_abs_shap_values_per_fold:\n",
    "        print(\"Warning: Failed to calculate any SHAP values. Skipping SHAP coarse selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    feature_importances_shap = pd.Series(np.mean(avg_abs_shap_values_per_fold, axis=0), index=X_filtered.columns).sort_values(ascending=False)\n",
    "    print(\"\\nFeature Importance (Average Absolute SHAP values across K-Folds):\")\n",
    "    print(feature_importances_shap)\n",
    "\n",
    "    num_features_to_keep = int(len(feature_importances_shap) * SHAP_COARSE_SELECTION_PERCENT)\n",
    "    if num_features_to_keep == 0 and len(feature_importances_shap) > 0: num_features_to_keep = 1\n",
    "    if num_features_to_keep > len(feature_importances_shap): num_features_to_keep = len(feature_importances_shap)\n",
    "\n",
    "    features_to_keep = feature_importances_shap.index[:num_features_to_keep].tolist()\n",
    "    features_to_drop = list(set(X_filtered.columns) - set(features_to_keep))\n",
    "\n",
    "    if features_to_drop:\n",
    "        print(f\"\\nBased on SHAP Coarse Selection (Retaining top {SHAP_COARSE_SELECTION_PERCENT * 100:.1f}%), the following features are removed: {features_to_drop}\")\n",
    "        X_shap_coarse = X_filtered[features_to_keep]\n",
    "    else:\n",
    "        print(\"\\nNo features removed after SHAP coarse selection.\")\n",
    "        X_shap_coarse = X_filtered.copy()\n",
    "\n",
    "    print(f\"Feature count after SHAP coarse selection: {X_shap_coarse.shape[1]}\")\n",
    "    return X_shap_coarse, features_to_drop\n",
    "\n",
    "\n",
    "def step3_wrapper_shap_iterative_selection(X_shap_coarse, y):\n",
    "    \"\"\"\n",
    "    Stage 3: Iterative feature refinement using Wrapper method with early stopping mechanism.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Stage 3: Iterative Feature Refinement ({KFOLD_SPLITS}-Fold CV) ---\")\n",
    "\n",
    "    if X_shap_coarse.shape[1] <= 1:\n",
    "        print(\"Insufficient feature count (<=1), cannot perform iterative refinement.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "    if X_shap_coarse.shape[0] < KFOLD_SPLITS:\n",
    "        print(f\"Sample size ({X_shap_coarse.shape[0]}) is less than K-folds ({KFOLD_SPLITS}). Cannot perform Cross-Validation.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    current_features = list(X_shap_coarse.columns)\n",
    "    best_features = list(current_features)\n",
    "\n",
    "    if PERFORMANCE_METRIC == 'r2':\n",
    "        best_score = -np.inf\n",
    "        is_better = lambda current, best: current > best\n",
    "    elif PERFORMANCE_METRIC == 'mae':\n",
    "        best_score = np.inf\n",
    "        is_better = lambda current, best: current < best\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported performance metric. Please select 'r2' or 'mae'.\")\n",
    "\n",
    "    performance_history = []\n",
    "    kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    # Check if sample size meets model minimum requirements\n",
    "    train_size_per_fold = X_shap_coarse.shape[0] * (KFOLD_SPLITS - 1) // KFOLD_SPLITS\n",
    "    min_samples_for_model = max(RF_MIN_SAMPLES_LEAF, RF_MIN_SAMPLES_SPLIT)\n",
    "    if train_size_per_fold < min_samples_for_model:\n",
    "        print(f\"Warning: Training set size per fold ({train_size_per_fold}) is smaller than RF model minimum requirement ({min_samples_for_model}). Skipping iterative refinement.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    print(\"Calculating baseline performance for initial feature set...\")\n",
    "    initial_fold_scores = []\n",
    "    for train_idx, val_idx in tqdm(kf.split(X_shap_coarse), total=KFOLD_SPLITS, desc=\"Baseline Evaluation Progress\"):\n",
    "        X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "        X_fold_val, y_fold_val = X_shap_coarse.iloc[val_idx][current_features], y.iloc[val_idx]\n",
    "        scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "        X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "        X_fold_val_scaled = scaler_fold.transform(X_fold_val)\n",
    "\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=RF_N_ESTIMATORS,\n",
    "            max_depth=RF_MAX_DEPTH,\n",
    "            min_samples_split=RF_MIN_SAMPLES_SPLIT,\n",
    "            min_samples_leaf=RF_MIN_SAMPLES_LEAF,\n",
    "            max_features=RF_MAX_FEATURES,\n",
    "            random_state=RANDOM_STATE,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        try:\n",
    "            rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "            y_pred = rf_model.predict(X_fold_val_scaled)\n",
    "            if PERFORMANCE_METRIC == 'r2':\n",
    "                initial_fold_scores.append(r2_score(y_fold_val, y_pred))\n",
    "            elif PERFORMANCE_METRIC == 'mae':\n",
    "                initial_fold_scores.append(mean_absolute_error(y_fold_val, y_pred))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Model training or prediction failed during baseline evaluation: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not initial_fold_scores:\n",
    "        print(\"Warning: Initial feature set failed to produce any valid scores. Cannot perform iterative refinement.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    best_score = np.mean(initial_fold_scores)\n",
    "    performance_history.append({'num_features': len(current_features), PERFORMANCE_METRIC: best_score})\n",
    "    print(f\"  Initial Feature Set ({len(current_features)} features), Avg {PERFORMANCE_METRIC.upper()}: {best_score:.4f}\")\n",
    "\n",
    "    print(\"Starting iterative feature removal...\")\n",
    "    \n",
    "    non_improvement_streak = 0\n",
    "    \n",
    "    while len(current_features) > 1:\n",
    "        current_iteration_shap_values_for_drop = []\n",
    "        for train_idx, val_idx in kf.split(X_shap_coarse):\n",
    "            X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "            scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "            X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "            \n",
    "            rf_model = RandomForestRegressor(\n",
    "                n_estimators=RF_N_ESTIMATORS,\n",
    "                max_depth=RF_MAX_DEPTH,\n",
    "                min_samples_split=RF_MIN_SAMPLES_SPLIT,\n",
    "                min_samples_leaf=RF_MIN_SAMPLES_LEAF,\n",
    "                max_features=RF_MAX_FEATURES,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            try:\n",
    "                rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                explainer = shap.TreeExplainer(rf_model)\n",
    "                shap_values_fold = explainer.shap_values(X_fold_train_scaled)\n",
    "                if shap_values_fold.ndim == 1:\n",
    "                    current_iteration_shap_values_for_drop.append(np.abs(shap_values_fold))\n",
    "                else:\n",
    "                    current_iteration_shap_values_for_drop.append(np.abs(shap_values_fold).mean(axis=0))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not current_iteration_shap_values_for_drop:\n",
    "            print(f\"  Warning: Iteration failed, unable to calculate SHAP values. Stopping iteration.\")\n",
    "            break\n",
    "\n",
    "        avg_iteration_shap = np.mean(current_iteration_shap_values_for_drop, axis=0)\n",
    "        feature_importances_current = pd.Series(avg_iteration_shap, index=current_features).sort_values(ascending=True)\n",
    "        feature_to_drop = feature_importances_current.index[0]\n",
    "        current_features.remove(feature_to_drop)\n",
    "        print(f\"  Removing least important feature: {feature_to_drop}\")\n",
    "\n",
    "        fold_scores_after_drop = []\n",
    "        for train_idx, val_idx in tqdm(kf.split(X_shap_coarse), total=KFOLD_SPLITS, desc=f\"Evaluating after removal\"):\n",
    "            X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "            X_fold_val, y_fold_val = X_shap_coarse.iloc[val_idx][current_features], y.iloc[val_idx]\n",
    "            scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "            X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "            X_fold_val_scaled = scaler_fold.transform(X_fold_val)\n",
    "            \n",
    "            rf_model = RandomForestRegressor(\n",
    "                n_estimators=RF_N_ESTIMATORS,\n",
    "                max_depth=RF_MAX_DEPTH,\n",
    "                min_samples_split=RF_MIN_SAMPLES_SPLIT,\n",
    "                min_samples_leaf=RF_MIN_SAMPLES_LEAF,\n",
    "                max_features=RF_MAX_FEATURES,\n",
    "                random_state=RANDOM_STATE,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            try:\n",
    "                rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                y_pred = rf_model.predict(X_fold_val_scaled)\n",
    "                if PERFORMANCE_METRIC == 'r2':\n",
    "                    fold_scores_after_drop.append(r2_score(y_fold_val, y_pred))\n",
    "                elif PERFORMANCE_METRIC == 'mae':\n",
    "                    fold_scores_after_drop.append(mean_absolute_error(y_fold_val, y_pred))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not fold_scores_after_drop:\n",
    "            print(f\"  Warning: Unable to calculate any valid scores after feature removal. Stopping iteration.\")\n",
    "            break\n",
    "\n",
    "        current_avg_score = np.mean(fold_scores_after_drop)\n",
    "        performance_history.append({'num_features': len(current_features), PERFORMANCE_METRIC: current_avg_score})\n",
    "        print(f\"  Feature Count: {len(current_features)}, Avg {PERFORMANCE_METRIC.upper()}: {current_avg_score:.4f}\")\n",
    "\n",
    "        if is_better(current_avg_score, best_score):\n",
    "            best_score, best_features = current_avg_score, list(current_features)\n",
    "            print(f\"    -> Performance improved, resetting patience. Current best feature set size: {len(best_features)}\")\n",
    "            non_improvement_streak = 0\n",
    "        else:\n",
    "            non_improvement_streak += 1\n",
    "            print(f\"    -> No improvement (Streak {non_improvement_streak}/{EARLY_STOPPING_PATIENCE})\")\n",
    "            if non_improvement_streak >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"    -> Performance has not improved for {EARLY_STOPPING_PATIENCE} consecutive rounds. Triggering early stopping.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nIterative refinement completed. Best feature set ({len(best_features)} features): {best_features}\")\n",
    "    return best_features, performance_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"File '{INPUT_FILE}' not found. Creating dummy data...\")\n",
    "        num_samples, feature_cols = 70, 162\n",
    "        df_list = [pd.DataFrame([f'Sample_{i+1}' for i in range(num_samples)], columns=['SampleID'])]\n",
    "        all_feature_names = [f'F{i+1}' for i in range(feature_cols)]\n",
    "        feature_data = np.random.rand(num_samples, len(all_feature_names))\n",
    "        df_features = pd.DataFrame(feature_data, columns=all_feature_names)\n",
    "        df_list.append(df_features)\n",
    "        df_dummy = pd.concat(df_list, axis=1)\n",
    "        y_values = 5 * df_dummy['F1'] + 3 * df_dummy['F2']**2 + np.random.randn(num_samples) * 0.5\n",
    "        df_dummy['Target'] = y_values\n",
    "        df_dummy.to_excel(INPUT_FILE, index=False)\n",
    "        print(f\"Dummy data created: '{INPUT_FILE}'.\")\n",
    "\n",
    "    X_full, y_full, df_original = load_data_from_excel()\n",
    "    if not X_full.empty:\n",
    "        original_corr_matrix = X_full.corr(method='pearson')\n",
    "        original_corr_path = os.path.join(OUTPUT_DIR, f'Original_Feature_Correlation_Matrix_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        original_corr_matrix.to_excel(original_corr_path, index=True)\n",
    "        print(f\"Saved: Original feature correlation matrix -> {original_corr_path}\")\n",
    "\n",
    "    X_filtered, dropped_pearson_list = step1_filter_high_correlated_features(X_full, y_full)\n",
    "    if not X_filtered.empty and X_filtered.shape[1] > 1:\n",
    "        filtered_corr_matrix = X_filtered.corr(method='pearson')\n",
    "        filtered_corr_path = os.path.join(OUTPUT_DIR, f'Stage1_Filtered_Feature_Correlation_Matrix_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        filtered_corr_matrix.to_excel(filtered_corr_path, index=True)\n",
    "        print(f\"Saved: Stage 1 filtered feature correlation matrix -> {filtered_corr_path}\")\n",
    "\n",
    "    X_shap_coarse, dropped_shap = step2_embed_shap_coarse_selection(X_filtered, y_full)\n",
    "    final_features, performance_history = step3_wrapper_shap_iterative_selection(X_shap_coarse, y_full)\n",
    "\n",
    "    if final_features:\n",
    "        print(\"\\n--- Sorting final features by original order ---\")\n",
    "        original_feature_order = X_full.columns\n",
    "        sorted_final_features = [col for col in original_feature_order if col in final_features]\n",
    "        final_features = sorted_final_features\n",
    "        print(f\"Final features sorted according to original column order.\")\n",
    "\n",
    "    print(f\"\\n--- Exporting final results to '{OUTPUT_DIR}' directory ---\")\n",
    "    num_dropped_pearson = len(dropped_pearson_list)\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Stage': ['Original Features', 'Stage 1: Pearson Correlation Filter', 'Stage 2: SHAP Coarse Selection', 'Stage 3: SHAP Iterative Refinement'],\n",
    "        'Feature Count': [X_full.shape[1], X_filtered.shape[1], X_shap_coarse.shape[1], len(final_features)],\n",
    "        'Features Removed': [0, num_dropped_pearson, len(dropped_shap), X_shap_coarse.shape[1] - len(final_features)]\n",
    "    })\n",
    "    summary_path = os.path.join(OUTPUT_DIR, f'Feature_Engineering_Process_Summary_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "    summary_df.to_excel(summary_path, index=False)\n",
    "    print(f\"Saved: Feature engineering process summary -> {summary_path}\")\n",
    "\n",
    "    history_path = os.path.join(OUTPUT_DIR, f'Performance_Iteration_History_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "    pd.DataFrame(performance_history).to_excel(history_path, index=False)\n",
    "    print(f\"Saved: Performance iteration history -> {history_path}\")\n",
    "\n",
    "    if len(final_features) > 1:\n",
    "        final_features_df = df_original[final_features]\n",
    "        final_corr_matrix = final_features_df.corr(method='pearson')\n",
    "        final_corr_path = os.path.join(OUTPUT_DIR, f'Final_Feature_Correlation_Matrix_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        final_corr_matrix.to_excel(final_corr_path, index=True)\n",
    "        print(f\"Saved: Final feature correlation matrix -> {final_corr_path}\")\n",
    "\n",
    "    if final_features:\n",
    "        cols_to_export = []\n",
    "        if not df_original.empty and df_original.columns[0] not in final_features and df_original.columns[0] != y_full.name:\n",
    "                cols_to_export.append(df_original.columns[0])\n",
    "        \n",
    "        cols_to_export.extend(final_features)\n",
    "        if y_full.name in df_original.columns:\n",
    "            cols_to_export.append(y_full.name)\n",
    "        \n",
    "        unique_cols_to_export = list(dict.fromkeys(cols_to_export))\n",
    "\n",
    "        dataset_path = os.path.join(OUTPUT_DIR, f'Final_Selected_Dataset_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        df_original[unique_cols_to_export].to_excel(dataset_path, index=False)\n",
    "        print(f\"Saved: Final selected dataset -> {dataset_path}\")\n",
    "    else:\n",
    "        print(\"No final features selected. Final dataset not saved.\")\n",
    "\n",
    "    print(\"\\nAll processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (matsci-ai)",
   "language": "python",
   "name": "matsci-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
